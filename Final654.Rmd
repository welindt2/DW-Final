---
title: "Final - EDLD 654"
author: "Dillon Welindt"
date: "Date"
output:
  html_document: default
  pdf_document: default
---
```{r setup}
#install.packages("EloRating", "cluster", "factoextra", "finalfit", "recipes", "tidyverse", "caret", "recipes", "tidyverse", "cutpointr", "initial_split", "gbm")
library(here)
library(EloRating)
library(tidyverse)
library(cluster)
library(factoextra)
library(finalfit)
library(recipes)
library(tidyverse)
library(caret)
library(recipes)
library(tidyverse)
library(cutpointr)
library(janitor)
library(ggplot2)

```


```{r}
mmaData <- read.csv(here("mmadata.csv"),
                       header=TRUE)

modernMMAData <- mmaData %>% filter(date>"2010-01-01")
str(mmaData)


#Generate unique fighter and opponent IDs
modernMMAData <- modernMMAData %>% mutate(fighterID=paste(date, fighter),
                                          opponentID=paste(date, opponent),
                                          result=if_else(result==0, "Loss", "Win"))

```

```{r}
#ff_glimpse(modernMMAData)
```

#Subset data to relevant predictors
```{r}
modernMMAData <- modernMMAData %>% mutate(reachDifference = reach_differential,
                                          ageDifference = age_differential,
                                          heightDifference = height_differential)
modernMMAData <- modernMMAData %>% dplyr::select(date, result, fighter, fighterID, opponent, stance, reachDifference, ageDifference, heightDifference, 124:535)
modernMMAData <- modernMMAData %>% dplyr::select(-dplyr::ends_with("differential", ignore.case = TRUE))%>% dplyr::select(-dplyr::ends_with("accuracy", ignore.case = TRUE)) %>% dplyr::select(-dplyr::starts_with("precomp", ignore.case = TRUE))
```


#Get elo ratings for fighters
```{r}
eloRatings <- modernMMAData %>% dplyr::select(date, result, fighter, fighterID, opponent)
eloRatings <- eloRatings %>% mutate(winner = ifelse(result=="Win", fighter, opponent),
                                    loser = ifelse(result=="Loss", fighter, opponent),
                                    date=as.Date(date))


seqcheck(winner = eloRatings$winner, loser = eloRatings$loser, Date = eloRatings$date)

res <- elo.seq(winner = eloRatings$winner, loser = eloRatings$loser, Date = eloRatings$date, runcheck = TRUE)
 
summary(res)

elos <- extract_elo(res, extractdate = eloRatings$date, IDs = eloRatings$fighter)
eloRatings <- eloRatings %>% mutate(elo=elos)

lastElo <- eloRatings %>% group_by(fighter) %>% mutate(precompElo=lag(elo)) %>% ungroup()
lastElo <- lastElo %>% dplyr::select(fighterID, precompElo)

modernMMAData <- left_join(
  modernMMAData,
  dplyr::select(lastElo, c('fighterID', 'precompElo')),
  by = c("fighterID" = "fighterID")
)

hist(lastElo$precompElo,
     xlab = "Elo Rating before Fight",
     ylab = "Frequency")

```

```{r}
#Prepare data for cluster analysis
new <- modernMMAData %>% dplyr::select(recent_avg_knockdowns:recent_avg_ground_strikes_attempts_per_min)
new <- new %>% filter(!is.na(recent_avg_body_strikes_landed_per_min))
scaledClusterDF <- scale(new)

#Check on appropriate number of clusters
#fviz_nbclust(scaledClusterDF, FUNcluster = kmeans, method = "silhouette")
#fviz_nbclust(scaledClusterDF, FUNcluster = kmeans, method = "wss")
#Will use 3 clusters

k <- kmeans(scaledClusterDF, centers = 3, iter.max = 20, nstart = 20)
cluster <- k$cluster
k <- as.character(k$cluster)
table(k)
barplot(table(k),
        xlab = "Cluster",
        ylab = "Count")

modernMMAData <- modernMMAData %>% filter(!is.na(recent_avg_body_strikes_landed_per_min))
modernMMAData <- modernMMAData %>% mutate(cluster = k)
```


```{r}
modernMMAData <- left_join(
  modernMMAData,
  modernMMAData,
  by = c("opponentID" = "fighterID"),
  suffix = c(".fighter", ".opponent")
)

#Add code for type of stance matchup
stances <- c("Orthodox", "Southpaw", "Switch")
modernMMAData <- modernMMAData %>% filter(stance.fighter %in% stances) %>% filter(stance.opponent %in% stances) %>% mutate(stanceMatchup = paste(stance.fighter, stance.opponent))

#Add code for type of cluster matchup
modernMMAData <- modernMMAData %>% mutate(clusterMatchup = paste(cluster.fighter, cluster.opponent))

#Add eloDifference
modernMMAData <- modernMMAData %>% mutate(eloDifference = precompElo.fighter-precompElo.opponent)

modernMMAData <- modernMMAData %>% clean_names() %>% data.frame()
modernMMAData <-modernMMAData %>% relocate(where(is.numeric), .after = where(is.character))
modernMMAData <- modernMMAData %>% filter(!is.na(precomp_elo_fighter)&!is.na(precomp_elo_opponent))

modernMMAData2 <- modernMMAData %>% select(-c(5:15))
```


#Split data into test and training
```{r}
set.seed(10312022)

loc      <- sample(1:nrow(modernMMAData2), round(nrow(modernMMAData2) * 0.8))
trainSet  <- data.frame(modernMMAData2[loc, ])
testSet  <- data.frame(modernMMAData2[-loc, ])
```

```{r}
finalfit::ff_glimpse(modernMMAData2)
```


#Recipe
```{r}


blueprint_mma <- recipe(x  = modernMMAData2,
                          vars  = colnames(modernMMAData2),
                          roles = c('id', 'outcome', 'id', 'id',rep('predictor',209))) %>%
  step_dummy('cluster_matchup',one_hot=TRUE) %>% 
  step_dummy('stance_matchup',one_hot=TRUE) %>% 
  step_impute_mean(all_numeric()) %>%
  step_zv(all_numeric())

```


```{r}
set.seed(10312022)
# Randomly shuffle the training dataset

  trainSet = trainSet[sample(nrow(trainSet)),]

# Create 10 folds with equal size

  folds = cut(seq(1,nrow(trainSet)),breaks=10,labels=FALSE)
  
# Create the list for each fold 
      
  my.indices <- vector('list',10)

  for(i in 1:10){
    my.indices[[i]] <- which(folds!=i)
  }


cv <- trainControl(method          = "cv",
                   index           = my.indices,
                   classProbs = TRUE,
                   summaryFunction = mnLogLoss)

grid <- data.frame(alpha = 0, lambda = c(seq(0,.1,.005))) 

noreg <- caret::train(blueprint_mma, 
                          data      = trainSet, 
                          method    = "glmnet",
                          family    = "binomial",
                          metric    = "logLoss",
                          trControl = cv)

ridge <- caret::train(blueprint_mma, 
                          data      = trainSet, 
                          method    = "glmnet",
                          family    = "binomial",
                          metric    = "logLoss",
                          trControl = cv,
                      tuneGrid = grid)

```


#Run predictions for nonregularized model
```{r}
predicted_test <- predict(noreg, testSet, type='prob')

cut.obj <- cutpointr(x     = predicted_test$Win,
                     class = testSet$result)

auc(cut.obj)
noregAUC<-auc(cut.obj)

pred_class <- ifelse(predicted_test$Win>.5,1,0)

confusion <- table(testSet$result,pred_class)

confusion

noregTPR<- confusion[2,2]/(confusion[2,1]+confusion[2,2])
noregTNR<- confusion[1,1]/(confusion[1,1]+confusion[1,2])
noregPRE<-confusion[2,2]/(confusion[1,2]+confusion[2,2])
noregACC<- (confusion[1,1]+confusion[2,2])/(confusion[1,1]+confusion[1,2]+confusion[2,1]+confusion[2,2])

noregTPR
noregTNR
noregPRE
noregLL<-noreg$results$logLoss



noregMetrics<-c(noregAUC, noregACC, noregTPR, noregTNR, noregPRE)

```

#Run predictions for ridge model
```{r}
predicted_test <- predict(ridge, testSet, type='prob')

cut.obj <- cutpointr(x     = predicted_test$Win,
                     class = testSet$result)

auc(cut.obj)
ridgeAUC<-auc(cut.obj)

pred_class <- ifelse(predicted_test$Win>.5,1,0)

confusion <- table(testSet$result,pred_class)

confusion

ridgeTPR<- confusion[2,2]/(confusion[2,1]+confusion[2,2])
ridgeTNR<- confusion[1,1]/(confusion[1,1]+confusion[1,2])
ridgePRE<-confusion[2,2]/(confusion[1,2]+confusion[2,2])
ridgeACC<- (confusion[1,1]+confusion[2,2])/(confusion[1,1]+confusion[1,2]+confusion[2,1]+confusion[2,2])

ridgeTPR
ridgeTNR
ridgePRE
ridgeLL<-ridge$results$logLoss



ridgeMetrics<-c(ridgeAUC, ridgeACC, ridgeTPR, ridgeTNR, ridgePRE)

```


#Gradient Boosted Forest

#Recipe
```{r}


blueprint_mma <- recipe(x  = modernMMAData2,
                          vars  = colnames(modernMMAData2),
                          roles = c('id', 'outcome', 'id', 'id',rep('predictor',209))) %>%
  step_dummy('cluster_matchup',one_hot=TRUE) %>% 
  step_dummy('stance_matchup',one_hot=TRUE) %>% 
  step_impute_mean(all_numeric()) %>%
  step_zv(all_numeric())



```

```{r}
set.seed(10312022)

loc      <- sample(1:nrow(modernMMAData2), round(nrow(modernMMAData2) * 0.8))
trainSet  <- data.frame(modernMMAData2[loc, ])
testSet  <- data.frame(modernMMAData2[-loc, ])
```

```{r}

set.seed(10312022)
# Randomly shuffle the training dataset

  trainSet = trainSet[sample(nrow(trainSet)),]

# Create 10 folds with equal size

  folds = cut(seq(1,nrow(trainSet)),breaks=10,labels=FALSE)
  
# Create the list for each fold 
      
  my.indices <- vector('list',10)

  for(i in 1:10){
    my.indices[[i]] <- which(folds!=i)
  }


cv <- trainControl(method          = "cv",
                   index           = my.indices,
                   classProbs = TRUE,
                   summaryFunction = mnLogLoss)


```

#Tune number of trees
```{r}
grid <- expand.grid(shrinkage         = 0.1,
                    n.trees           = 1:1000,
                    interaction.depth = 5,
                    n.minobsinnode    = 10)


gbm1 <- caret::train(blueprint_mma,
                     data         = trainSet,
                     method       = 'gbm',
                     trControl    = cv,
                     tuneGrid     = grid,
                     bag.fraction = 1,
                     metric       = 'logLoss')

plot(gbm1,type='l')

gbm1$bestTune

```

```{r}
grid <- expand.grid(shrinkage         = 0.1,
                    n.trees           = 136,
                    interaction.depth = 1:15,
                    n.minobsinnode    = c(5,10,20,30,40,50))

gbm2 <- caret::train(blueprint_mma,
                     data         = trainSet,
                     method       = 'gbm',
                     trControl    = cv,
                     tuneGrid     = grid,
                     bag.fraction = 1,
                     metric       = 'logLoss')

plot(gbm2)

gbm2$bestTune
```

```{r}
grid <- expand.grid(shrinkage         = 0.01,
                    n.trees           = 1:10000,
                    interaction.depth = 10,
                    n.minobsinnode    = 10)


gbm3 <- caret::train(blueprint_mma,
                     data         = trainSet,
                     method       = 'gbm',
                     trControl    = cv,
                     tuneGrid     = grid,
                     bag.fraction = 1,
                     metric       = 'logLoss')

plot(gbm3,type='l')

gbm3$bestTune

```

#Tune bag fraction
```{r}
grid <- expand.grid(shrinkage         = 0.01,
                    n.trees           = 1162,
                    interaction.depth = 10,
                    n.minobsinnode    = 10)

bag.fr <- seq(0.1,1,.05)

my.models <- vector('list',length(bag.fr))

for(i in 1:length(bag.fr)){
  
  my.models[[i]] <- caret::train(blueprint_mma,
                                 data      = trainSet,
                                 method    = 'gbm',
                                 trControl = cv,
                                 tuneGrid  = grid,
                                 bag.fraction = bag.fr[i])
}
cv.LogL <- c()

for(i in 1:length(bag.fr)){
  cv.LogL[i] <- my.models[[i]]$results$logLoss
}

ggplot()+
  geom_line(aes(x=bag.fr,y=cv.LogL))+
  theme_bw()+
  xlab('Bag Fraction')+
  ylab('LogLoss (Cross-validated)')+
  scale_x_continuous(breaks = bag.fr)

bag.fr[which.min(cv.LogL)]

#.55
```

```{r}
grid <- expand.grid(shrinkage         = 0.01,
                    n.trees           = 1162,
                    interaction.depth = 10,
                    n.minobsinnode    = 10)
  
  gbmModel <- caret::train(blueprint_mma,
                                 data      = trainSet,
                                 method    = 'gbm',
                                 trControl = cv,
                                 tuneGrid  = grid,
                                 bag.fraction = .55)

#.55
```


#Predict test set
```{r}

# Predict the probabilities for the observations in the test dataset

predicted_test <- predict(gbmModel, testSet, type='prob')

head(predicted_test)
```

```{r}
require(cutpointr)

cut.objGBM <- cutpointr(x     = predicted_test$Win,
                     class = testSet$result)

gbmAUC <- auc(cut.obj)


pred_class <- ifelse(predicted_test$Win>.5,1,0)

confusion <- table(testSet$result,pred_class)

confusion[1,1]/(confusion[1,1]+confusion[1,2])

confusion[1,2]/(confusion[1,1]+confusion[1,2])


confusion[2,2]/(confusion[2,1]+confusion[2,2])


confusion[2,2]/(confusion[1,2]+confusion[2,2])

confusion
```

```{r}
gbmTPR<- confusion[2,2]/(confusion[2,1]+confusion[2,2])
gbmTNR<- confusion[1,1]/(confusion[1,1]+confusion[1,2])
gbmPRE<-confusion[2,2]/(confusion[1,2]+confusion[2,2])
gbmACC<- (confusion[1,1]+confusion[2,2])/(confusion[1,1]+confusion[1,2]+confusion[2,1]+confusion[2,2])

gbmTPR
gbmTNR
gbmPRE
gbmLL<-ridge$results$logLoss


gbmAUC
gbmMetrics<-c(gbmAUC, gbmACC, gbmTPR, gbmTNR, gbmPRE)

```

```{r}
performanceTable2<-rbind(noregMetrics,ridgeMetrics, gbmMetrics)
colnames(performanceTable2)<-c("AUC","ACC", "TPR", "TNR", "PRE")
rownames(performanceTable2)<-c("Logistic Regression", "Logistic Regression with Ridge Penalty", "Gradient Boosted")
performanceTable2
```


```{r}
citation(package = "base", lib.loc = NULL, auto = NULL)
citation(package = "here")

citation("EloRating")
citation("cluster")
citation("factoextra")
citation("finalfit")
citation("recipes")
citation("tidyverse")
citation("caret")
citation("recipes")
citation("tidyverse")
citation("cutpointr")
citation("gbm")


plot_roc(cut.objGBM$roc_curve[[1]])

vip::vip(gbmModel)

```

